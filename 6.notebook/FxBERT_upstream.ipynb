{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq_oqA_b1pVi"
      },
      "source": [
        "## 0.Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VQszBadJhSdG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os, sys\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import random, math, operator\n",
        "from pprint import pprint\n",
        "import pickle, gc, itertools\n",
        "from pathlib import Path\n",
        "import time, datetime\n",
        "from collections import *\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import gc\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.utils import *\n",
        "from keras.layers import LSTM\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YlsmjaOELqx"
      },
      "source": [
        "## 1.Setup configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q-T6aMqBtt1N",
        "outputId": "3e1ea5c4-170f-4fcc-a047-1616ec641d4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FxBERT_upstream.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "from requests import get\n",
        "#colab_filename = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
        "colab_filename = 'FxBERT_upstream.ipynb'\n",
        "result_filename = colab_filename.replace('.ipynb', '.csv')\n",
        "result_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tVF9I76bYWw4"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    BATCH_SIZE = 128\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEAD = 8  # used in bert model\n",
        "    FF_DIM = 1024  # used in bert model\n",
        "    NUM_LAYERS = 12\n",
        "    EPOCH = 3 # 10000\n",
        "    SEQLENGTH = 100\n",
        "    CURRENCY ='USD_JPY_M15'\n",
        "    MASK_PERCT = 0.15\n",
        "    maxlen = 100\n",
        "    patience = 30\n",
        "    seqlen = 10\n",
        "    isTrain = True\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQhMMnNnFjsc",
        "outputId": "dcde6e8f-2ff4-4d5f-da9b-9dae7e99ae81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_folder = '/content/drive/MyDrive/TradingBERT/'\n",
        "\n",
        "gd_rawdata_folder = base_folder + '1.rawData/'\n",
        "gd_ref_folder = base_folder + '2.refData/'\n",
        "gd_processedData_folder = base_folder + '3.processedData/'\n",
        "gd_mdl_folder = base_folder + '4.model/downstream/'+config.CURRENCY+'/'\n",
        "gd_mdl_upstream_folder = base_folder + '4.model/upstream/'\n",
        "gd_result_folder = base_folder + '5.result/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Path(gd_processedData_folder).mkdir(parents=True, exist_ok=True)\n",
        "Path(gd_ref_folder).mkdir(parents=True, exist_ok=True)\n",
        "Path(gd_mdl_folder).mkdir(parents=True, exist_ok=True)\n",
        "Path(gd_result_folder).mkdir(parents=True, exist_ok=True)\n",
        "# Path(gd_rawdata_folder).mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "AFiKKX8ws7he"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JwlBpJre5K3"
      },
      "source": [
        "## 2.FxBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykQTb_Q8qgbg"
      },
      "source": [
        "### 2.1 Read tfrecords - upstream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "sGOydTO8G6dS"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "def read_dataset_Fintech(channel, channel_name, maxlen):\n",
        "\n",
        "    def _parse_seq_function(example_proto):\n",
        "\n",
        "        seq_feature_description = {\n",
        "            'isNext': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "            'segment_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "            'input_ids_ori': tf.io.FixedLenFeature([], tf.string),\n",
        "            'sample_weight': tf.io.FixedLenFeature([], tf.string),\n",
        "            'segment_ids_3num': tf.io.FixedLenFeature([], tf.string),\n",
        "        }\n",
        "\n",
        "        features = tf.io.parse_single_example(example_proto, seq_feature_description)\n",
        "\n",
        "        # upstream\n",
        "        x_input_ids = tf.io.decode_raw(features['input_ids'], tf.int64)\n",
        "        x_input_ids = tf.reshape(x_input_ids, [maxlen,6])\n",
        "        x_input_ids_candle, x_input_ids_pos, x_input_ids_die, x_input_ids_cle, x_input_ids_ule, x_input_ids_lle = \\\n",
        "                        tf.split(x_input_ids, 6, axis=1)\n",
        "        x_input_ids_candle, x_input_ids_pos, x_input_ids_die, x_input_ids_cle, x_input_ids_ule, x_input_ids_lle = \\\n",
        "                        tf.squeeze(x_input_ids_candle, axis=1), tf.squeeze(x_input_ids_pos, axis=1), \\\n",
        "                        tf.squeeze(x_input_ids_die, axis=1), tf.squeeze(x_input_ids_cle, axis=1), \\\n",
        "                        tf.squeeze(x_input_ids_ule, axis=1), tf.squeeze(x_input_ids_lle, axis=1)\n",
        "\n",
        "        x_segment_ids = tf.io.decode_raw(features['segment_ids_3num'], tf.int64)\n",
        "        x_segment_ids = tf.reshape(x_segment_ids, [maxlen,])\n",
        "\n",
        "        out_input_ids = tf.io.decode_raw(features['input_ids_ori'], tf.int64)\n",
        "        out_input_ids = tf.reshape(out_input_ids, [maxlen,6])\n",
        "        out_input_ids_candle, out_input_ids_pos, out_input_ids_die, out_input_ids_cle, out_input_ids_ule, out_input_ids_lle = \\\n",
        "                        tf.split(out_input_ids, 6, axis=1)\n",
        "        out_input_ids_candle, out_input_ids_pos, out_input_ids_die, out_input_ids_cle, out_input_ids_ule, out_input_ids_lle = \\\n",
        "                        tf.squeeze(out_input_ids_candle, axis=1), tf.squeeze(out_input_ids_pos, axis=1), \\\n",
        "                        tf.squeeze(out_input_ids_die, axis=1), tf.squeeze(out_input_ids_cle, axis=1), \\\n",
        "                        tf.squeeze(out_input_ids_ule, axis=1), tf.squeeze(out_input_ids_lle, axis=1)\n",
        "\n",
        "        label_isNext = features['isNext']\n",
        "\n",
        "        s_weight = tf.io.decode_raw(features['sample_weight'], tf.int64)\n",
        "        s_weight = tf.reshape(s_weight, [maxlen,])\n",
        "\n",
        "        inputs = {}\n",
        "        inputs['input_ids_candle'] = x_input_ids_candle\n",
        "        inputs['input_ids_pos'] = x_input_ids_pos\n",
        "        inputs['input_ids_die'] = x_input_ids_die\n",
        "        inputs['input_ids_cle'] = x_input_ids_cle\n",
        "        inputs['input_ids_ule'] = x_input_ids_ule\n",
        "        inputs['input_ids_lle'] = x_input_ids_lle\n",
        "        inputs['segment_ids'] = x_segment_ids\n",
        "\n",
        "        targets = {}\n",
        "        targets['out_input_ids_candle'] = out_input_ids_candle\n",
        "        targets['out_input_ids_pos'] = out_input_ids_pos\n",
        "        targets['out_input_ids_die'] = out_input_ids_die\n",
        "        targets['out_input_ids_cle'] = out_input_ids_cle\n",
        "        targets['out_input_ids_ule'] = out_input_ids_ule\n",
        "        targets['out_input_ids_lle'] = out_input_ids_lle\n",
        "        targets['label_isNext'] = label_isNext\n",
        "\n",
        "        sample_weights = {}\n",
        "        sample_weights['out_input_ids_candle'] = s_weight\n",
        "        sample_weights['out_input_ids_pos'] = s_weight\n",
        "        sample_weights['out_input_ids_die'] = s_weight\n",
        "        sample_weights['out_input_ids_cle'] = s_weight\n",
        "        sample_weights['out_input_ids_ule'] = s_weight\n",
        "        sample_weights['out_input_ids_lle'] = s_weight\n",
        "        sample_weights['label_isNext'] = tf.ones_like(label_isNext)\n",
        "\n",
        "        return inputs, targets, sample_weights\n",
        "\n",
        "    filenames = [os.path.join(channel, c) for c in channel_name]\n",
        "    print(filenames)\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "\n",
        "    dataset = dataset.map(_parse_seq_function, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.shuffle(buffer_size=4000000)\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "    dataset = dataset.batch(config.BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQirCnScdpO8",
        "outputId": "10b7fb63-3ed8-4c60-a6cf-87a564f3b83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/MyResearch_FT/14.BERT/11.github/TradingBERT/3.processedData/FxBERT_upstream_AUD_USD_M15_sample.tfrecords']\n"
          ]
        }
      ],
      "source": [
        "upstream_tfrecordfile = [gd_processedData_folder+'FxBERT_upstream_AUD_USD_M15_sample.tfrecords']\n",
        "ds_train = read_dataset_Fintech('', upstream_tfrecordfile, config.maxlen)\n",
        "\n",
        "total_batches = sum(1 for _ in ds_train)\n",
        "upstream_valid = ds_train.skip(total_batches - 1).take(1)\n",
        "upstream_train = ds_train.take(total_batches - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2KFTTv1zZYt"
      },
      "source": [
        "### 2.2 basic functions\n",
        "code ref for BERT_module: https://keras.io/examples/nlp/masked_language_modeling/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "vKqurfiHe5K5"
      },
      "outputs": [],
      "source": [
        "def bert_module(query, key, value, i, pad_mask=None):\n",
        "    # Multi headed self-attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=config.NUM_HEAD,\n",
        "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "        name=\"encoder_{}/multiheadattention\".format(i),\n",
        "    )(query, key, value, attention_mask=pad_mask)\n",
        "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
        "        attention_output\n",
        "    )\n",
        "    attention_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
        "    )(query + attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ffn = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "            layers.Dense(config.EMBED_DIM),\n",
        "        ],\n",
        "        name=\"encoder_{}/ffn\".format(i),\n",
        "    )\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
        "        ffn_output\n",
        "    )\n",
        "    sequence_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
        "    )(attention_output + ffn_output)\n",
        "\n",
        "    return sequence_output\n",
        "\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc\n",
        "\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "   batch_size, len_q = seq_q.get_shape()\n",
        "   batch_size, len_k = seq_k.get_shape()\n",
        "   # eq(zero) is PAD token\n",
        "   pad_attn_mask = tf.equal(seq_k, 0)\n",
        "   pad_attn_mask = tf.expand_dims(pad_attn_mask, 1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "   pad_attn_mask = tf.repeat(pad_attn_mask, repeats=[len_q], axis=1)\n",
        "   return pad_attn_mask  # batch_size x len_q x len_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBprLgEQzVmh"
      },
      "source": [
        "### 2.3 create FxBERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lk1D2bxw6H6-"
      },
      "outputs": [],
      "source": [
        "candlestick_vocab_size=39083\n",
        "pos_vocab_size=6109\n",
        "direction_vocab_size=7\n",
        "candlepiece_vocab_size=138\n",
        "segment_size=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AVv60pVyzUmR"
      },
      "outputs": [],
      "source": [
        "def create_FxBERT(maxlen):\n",
        "    input_wtoken_candle = layers.Input((maxlen,), dtype=tf.int64, name='input_ids_candle')\n",
        "    input_wtoken_pos = layers.Input((maxlen,), dtype=tf.int64, name='input_ids_pos')\n",
        "    input_wtoken_die = layers.Input((maxlen,), dtype=tf.int64, name='input_ids_die')\n",
        "    input_wtoken_cle = layers.Input((maxlen,), dtype=tf.int64, name='input_ids_cle')\n",
        "    input_wtoken_ule = layers.Input((maxlen,), dtype=tf.int64, name='input_ids_ule')\n",
        "    input_wtoken_lle = layers.Input((maxlen,), dtype=tf.int64, name='input_ids_lle')\n",
        "\n",
        "    input_segment = layers.Input((maxlen,), dtype=tf.int64, name='segment_ids')\n",
        "\n",
        "    wtoken_candle_embeddings = layers.Embedding(candlestick_vocab_size, config.EMBED_DIM, name='wtoken_candle_embedding')(input_wtoken_candle)\n",
        "    wtoken_pos_embeddings = layers.Embedding(pos_vocab_size, config.EMBED_DIM, name='wtoken_embedding')(input_wtoken_pos)\n",
        "    wtoken_die_embeddings = layers.Embedding(direction_vocab_size, config.EMBED_DIM, name='direction_embedding')(input_wtoken_die)\n",
        "    wtoken_cle_embeddings = layers.Embedding(candlepiece_vocab_size, config.EMBED_DIM, name='candlelen_embedding')(input_wtoken_cle)\n",
        "    wtoken_ule_embeddings = layers.Embedding(candlepiece_vocab_size, config.EMBED_DIM, name='upperlen_embedding')(input_wtoken_ule)\n",
        "    wtoken_lle_embeddings = layers.Embedding(candlepiece_vocab_size, config.EMBED_DIM, name='lowerlen_embedding')(input_wtoken_lle)\n",
        "\n",
        "    segment_embeddings = layers.Embedding(segment_size, config.EMBED_DIM, name='segment_embedding')(input_segment)\n",
        "\n",
        "    mask = layers.Embedding(candlestick_vocab_size, config.EMBED_DIM, mask_zero=True).compute_mask(input_wtoken_candle)\n",
        "    attn_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "\n",
        "    position_embeddings = layers.Embedding(\n",
        "        input_dim=maxlen,\n",
        "        output_dim=config.EMBED_DIM,\n",
        "        weights=[get_pos_encoding_matrix(maxlen, config.EMBED_DIM)],\n",
        "        name=\"position_embedding\")(tf.range(start=0, limit=maxlen, delta=1))\n",
        "\n",
        "    embeddings = wtoken_candle_embeddings + wtoken_pos_embeddings + position_embeddings + segment_embeddings + \\\n",
        "                    wtoken_die_embeddings + wtoken_cle_embeddings + wtoken_ule_embeddings + wtoken_lle_embeddings\n",
        "\n",
        "    encoder_input = embeddings\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        encoder_output = bert_module(encoder_input, encoder_input, encoder_input, i, attn_mask)\n",
        "\n",
        "    output_wtoken_candle = layers.Dense(candlestick_vocab_size, name='out_input_ids_candle', activation='softmax')(encoder_output)\n",
        "    output_wtoken_pos = layers.Dense(pos_vocab_size, name='out_input_ids_pos', activation='softmax')(encoder_output)\n",
        "    output_wtoken_die = layers.Dense(direction_vocab_size, name='out_input_ids_die', activation='softmax')(encoder_output)\n",
        "    output_wtoken_cle = layers.Dense(candlepiece_vocab_size, name='out_input_ids_cle', activation='softmax')(encoder_output)\n",
        "    output_wtoken_ule = layers.Dense(candlepiece_vocab_size, name='out_input_ids_ule', activation='softmax')(encoder_output)\n",
        "    output_wtoken_lle = layers.Dense(candlepiece_vocab_size, name='out_input_ids_lle', activation='softmax')(encoder_output)\n",
        "\n",
        "    flat_encoder_output = layers.Flatten(name=\"flat_encoder_output\")(encoder_output)\n",
        "    output_label_isNext = layers.Dense(2, name='label_isNext', activation='softmax')(flat_encoder_output)\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "                            inputs=[input_wtoken_candle, input_wtoken_pos, input_wtoken_die, input_wtoken_cle,\n",
        "                                    input_wtoken_ule, input_wtoken_lle, input_segment],\n",
        "                            outputs=[output_wtoken_candle, output_wtoken_pos, output_wtoken_die, output_wtoken_cle,\n",
        "                                     output_wtoken_ule, output_wtoken_lle, output_label_isNext],\n",
        "                            name='FxBERT')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IudQmaRe5K7"
      },
      "source": [
        "### 2.4 Train - upstream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lAhz446Fgvu"
      },
      "source": [
        "#### 2.4.1 filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LMSv_jjV2SfU"
      },
      "outputs": [],
      "source": [
        "savepath_FxBERT = gd_mdl_upstream_folder+colab_filename.replace('.ipynb', '_'+config.CURRENCY+'.h5')\n",
        "savepath_FxBERT_weights = savepath_FxBERT.replace('.h5', '_weights.h5')\n",
        "savepath_FxBERT_checkpoint = savepath_FxBERT.replace('.h5', '_cp.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "04Tc4l-JUaYh"
      },
      "outputs": [],
      "source": [
        "def learning_rate_schedule(epoch, lr, current_lr=None):\n",
        "    warmup_epochs = 20\n",
        "    peak_lr = 1e-4\n",
        "    total_epochs = config.EPOCH\n",
        "\n",
        "    if current_lr:  # If resuming training, set warmup_lr to current_lr\n",
        "        warmup_lr = current_lr\n",
        "    else:\n",
        "        warmup_lr = 1e-6\n",
        "\n",
        "    if epoch < warmup_epochs:\n",
        "        return warmup_lr + (peak_lr - warmup_lr) * (epoch / warmup_epochs)\n",
        "    else:\n",
        "        decay_factor = max(0, (total_epochs - epoch) / (total_epochs - warmup_epochs))  # Ensure non-negative\n",
        "        return peak_lr * decay_factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flZ6V24MGhKr"
      },
      "source": [
        "#### 2.4.2 training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMQnrHZdsVBI",
        "outputId": "2469238c-dbdd-4c66-dd0c-59b29c0ef8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FxBERT training upstream for USD_JPY_M15\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 1e-06.\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "576/576 - 130s - loss: 1.8373 - out_input_ids_candle_loss: 0.3033 - out_input_ids_pos_loss: 0.2506 - out_input_ids_die_loss: 0.0609 - out_input_ids_cle_loss: 0.1573 - out_input_ids_ule_loss: 0.1598 - out_input_ids_lle_loss: 0.1797 - label_isNext_loss: 0.7258 - out_input_ids_candle_accuracy: 1.8898e-05 - out_input_ids_pos_accuracy: 1.2284e-04 - out_input_ids_die_accuracy: 0.2541 - out_input_ids_cle_accuracy: 5.2442e-04 - out_input_ids_ule_accuracy: 1.7953e-04 - out_input_ids_lle_accuracy: 5.3859e-04 - label_isNext_accuracy: 0.5003 - val_loss: 1.8087 - val_out_input_ids_candle_loss: 0.3052 - val_out_input_ids_pos_loss: 0.2526 - val_out_input_ids_die_loss: 0.0574 - val_out_input_ids_cle_loss: 0.1575 - val_out_input_ids_ule_loss: 0.1605 - val_out_input_ids_lle_loss: 0.1795 - val_label_isNext_loss: 0.6959 - val_out_input_ids_candle_accuracy: 0.0000e+00 - val_out_input_ids_pos_accuracy: 0.0000e+00 - val_out_input_ids_die_accuracy: 0.3172 - val_out_input_ids_cle_accuracy: 0.0000e+00 - val_out_input_ids_ule_accuracy: 0.0000e+00 - val_out_input_ids_lle_accuracy: 0.0000e+00 - val_label_isNext_accuracy: 0.4500 - lr: 1.0000e-06 - 130s/epoch - 226ms/step\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 5.950000000000001e-06.\n",
            "Epoch 2/3\n",
            "576/576 - 94s - loss: 1.7407 - out_input_ids_candle_loss: 0.2974 - out_input_ids_pos_loss: 0.2486 - out_input_ids_die_loss: 0.0450 - out_input_ids_cle_loss: 0.1438 - out_input_ids_ule_loss: 0.1390 - out_input_ids_lle_loss: 0.1598 - label_isNext_loss: 0.7071 - out_input_ids_candle_accuracy: 0.0032 - out_input_ids_pos_accuracy: 2.5040e-04 - out_input_ids_die_accuracy: 0.3761 - out_input_ids_cle_accuracy: 0.0027 - out_input_ids_ule_accuracy: 0.0356 - out_input_ids_lle_accuracy: 0.0061 - label_isNext_accuracy: 0.4982 - val_loss: 1.6841 - val_out_input_ids_candle_loss: 0.2963 - val_out_input_ids_pos_loss: 0.2504 - val_out_input_ids_die_loss: 0.0376 - val_out_input_ids_cle_loss: 0.1359 - val_out_input_ids_ule_loss: 0.1231 - val_out_input_ids_lle_loss: 0.1430 - val_label_isNext_loss: 0.6977 - val_out_input_ids_candle_accuracy: 0.0034 - val_out_input_ids_pos_accuracy: 0.0000e+00 - val_out_input_ids_die_accuracy: 0.4296 - val_out_input_ids_cle_accuracy: 0.0069 - val_out_input_ids_ule_accuracy: 0.1065 - val_out_input_ids_lle_accuracy: 0.0241 - val_label_isNext_accuracy: 0.5000 - lr: 5.9500e-06 - 94s/epoch - 163ms/step\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 1.0900000000000002e-05.\n",
            "Epoch 3/3\n",
            "576/576 - 90s - loss: 1.5942 - out_input_ids_candle_loss: 0.2827 - out_input_ids_pos_loss: 0.2442 - out_input_ids_die_loss: 0.0341 - out_input_ids_cle_loss: 0.1141 - out_input_ids_ule_loss: 0.0959 - out_input_ids_lle_loss: 0.1121 - label_isNext_loss: 0.7111 - out_input_ids_candle_accuracy: 0.0325 - out_input_ids_pos_accuracy: 0.0330 - out_input_ids_die_accuracy: 0.4375 - out_input_ids_cle_accuracy: 0.0719 - out_input_ids_ule_accuracy: 0.2404 - out_input_ids_lle_accuracy: 0.1251 - label_isNext_accuracy: 0.4980 - val_loss: 1.5176 - val_out_input_ids_candle_loss: 0.2697 - val_out_input_ids_pos_loss: 0.2386 - val_out_input_ids_die_loss: 0.0300 - val_out_input_ids_cle_loss: 0.0986 - val_out_input_ids_ule_loss: 0.0804 - val_out_input_ids_lle_loss: 0.0859 - val_label_isNext_loss: 0.7144 - val_out_input_ids_candle_accuracy: 0.0810 - val_out_input_ids_pos_accuracy: 0.0915 - val_out_input_ids_die_accuracy: 0.4437 - val_out_input_ids_cle_accuracy: 0.1162 - val_out_input_ids_ule_accuracy: 0.2254 - val_out_input_ids_lle_accuracy: 0.3028 - val_label_isNext_accuracy: 0.4700 - lr: 1.0900e-05 - 90s/epoch - 156ms/step\n"
          ]
        }
      ],
      "source": [
        "if config.isTrain:\n",
        "    print(\"FxBERT training upstream for\", config.CURRENCY)\n",
        "\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(savepath_FxBERT_checkpoint, monitor=\"val_loss\", verbose=0, mode=\"min\",\n",
        "                                                            save_best_only=True, save_weights_only=False, save_freq=\"epoch\" )\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=config.patience, verbose=1, mode=\"min\", restore_best_weights=True)\n",
        "\n",
        "    FxBERT = create_FxBERT(config.maxlen)\n",
        "    opt = keras.optimizers.Adam()\n",
        "    lr_callback = LearningRateScheduler(learning_rate_schedule, verbose=1)\n",
        "\n",
        "    FxBERT.compile(\n",
        "        optimizer=opt,\n",
        "        loss={\n",
        "                \"out_input_ids_candle\": keras.losses.SparseCategoricalCrossentropy(),\n",
        "                \"out_input_ids_pos\": keras.losses.SparseCategoricalCrossentropy(),\n",
        "                \"out_input_ids_die\": keras.losses.SparseCategoricalCrossentropy(),\n",
        "                \"out_input_ids_cle\": keras.losses.SparseCategoricalCrossentropy(),\n",
        "                \"out_input_ids_ule\": keras.losses.SparseCategoricalCrossentropy(),\n",
        "                \"out_input_ids_lle\": keras.losses.SparseCategoricalCrossentropy(),\n",
        "                \"label_isNext\": keras.losses.SparseCategoricalCrossentropy(),\n",
        "            },\n",
        "        weighted_metrics={\n",
        "                \"out_input_ids_candle\": 'accuracy',\n",
        "                \"out_input_ids_pos\": 'accuracy',\n",
        "                \"out_input_ids_die\": 'accuracy',\n",
        "                \"out_input_ids_cle\": 'accuracy',\n",
        "                \"out_input_ids_ule\": 'accuracy',\n",
        "                \"out_input_ids_lle\": 'accuracy',\n",
        "                \"label_isNext\": 'accuracy',\n",
        "            },\n",
        "        )\n",
        "\n",
        "    FxBERT.fit(\n",
        "        upstream_train,\n",
        "        validation_data=upstream_valid,\n",
        "        epochs=config.EPOCH,\n",
        "        callbacks=[lr_callback, checkpoint_cb, early_stop],\n",
        "        verbose=2)\n",
        "\n",
        "    FxBERT.save(savepath_FxBERT)\n",
        "    FxBERT.save_weights(savepath_FxBERT_weights)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "J2KFTTv1zZYt",
        "iBprLgEQzVmh"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "gpuClass": "premium",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}